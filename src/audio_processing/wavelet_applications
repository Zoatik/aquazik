import librosa
import numpy as np
from freq_analysis import Tools
import matplotlib.pyplot as plt

WINDOW_TIME = 0.01
FREQ_MIN = 100
FREQ_MAX = 7000

def estimate_bpm_from_wavelet(audio_data, sample_rate, use_vqt=False, bins_per_octave=12):
    """
    Estime le BPM en s'appuyant sur une transformée type wavelet (CQT/VQT).
    Met à jour audio_bpm et retourne (bpm_estime, beat_times_seconds).
    """
    n_bins = bins_per_octave * 9         # C0..B8
    fmin = librosa.note_to_hz("C0")
    hop_length = max(1, int(sample_rate * WINDOW_TIME))

    # 1) CQT/VQT
    if use_vqt:
        C = librosa.vqt(audio_data, sr=sample_rate, hop_length=hop_length,
                        fmin=fmin, n_bins=n_bins, bins_per_octave=bins_per_octave)
    else:
        C = librosa.cqt(audio_data, sr=sample_rate, hop_length=hop_length,
                        fmin=fmin, n_bins=n_bins, bins_per_octave=bins_per_octave)

    # 2) Magnitude + log-compression (meilleur SNR pour l'onset)
    S = np.abs(C)
    if S.size == 0 or np.max(S) == 0:
        return audio_bpm, np.array([])

    S_db = librosa.amplitude_to_db(S, ref=np.max)

    # 3) Enveloppe d'attaques (agrégation sur les hauteurs)
    onset_env = librosa.onset.onset_strength(
        S=S_db, sr=sample_rate, hop_length=hop_length, aggregate=np.mean
    )

    # 4) Estimation de tempo + temps des battements
    tempo, beat_frames = librosa.beat.beat_track(
        onset_envelope=onset_env, sr=sample_rate, hop_length=hop_length
    )
    beat_times = librosa.frames_to_time(beat_frames, sr=sample_rate, hop_length=hop_length)

    # 5) Option de “snapping” esthétique (comme tu faisais)
    est_bpm = float(tempo)
    lower_bpm = np.floor(est_bpm) 
    upper_bpm = lower_bpm + 2
    if abs(est_bpm - lower_bpm) > abs(est_bpm - upper_bpm):
        est_bpm = upper_bpm
    else:
        est_bpm = lower_bpm
    

    audio_bpm = est_bpm
    return est_bpm, beat_times

def wavelet_mag(audio_data, sample_rate, use_vqt=False, bins_per_octave=12):
    """Retourne (mag, times, note_labels) où:
    - mag: matrice (n_bins, n_frames) normalisée [0..1]
    - times: centres temporels des frames (en secondes)
    - note_labels: liste de labels 'C0'..'B8' alignés aux lignes de mag
    """
    n_bins = bins_per_octave * 9           # octaves 0..8 → 9 * 12 = 108
    fmin = librosa.note_to_hz("C0")
    hop_length = max(1, int(sample_rate * WINDOW_TIME))

    if use_vqt:
        C = librosa.vqt(audio_data, sr=sample_rate, hop_length=hop_length,
                        fmin=fmin, n_bins=n_bins, bins_per_octave=bins_per_octave)
    else:
        C = librosa.cqt(audio_data, sr=sample_rate, hop_length=hop_length,
                        fmin=fmin, n_bins=n_bins, bins_per_octave=bins_per_octave)

    mag = np.abs(C)
    mx = np.max(mag) if mag.size else 0.0
    mag = mag / mx if mx > 0 else mag

    # Temps au centre de chaque frame
    times = librosa.frames_to_time(np.arange(mag.shape[1]), sr=sample_rate, hop_length=hop_length)

    # Labels de notes pour chaque bin CQT
    freqs = librosa.cqt_frequencies(n_bins=n_bins, fmin=fmin, bins_per_octave=bins_per_octave)
    note_labels = [Tools.freq_to_note(f) for f in freqs]  # ex: 'C#4', 'A3', ...

    return mag, times, note_labels



def fft_note_buckets_mag(audio_data, sample_rate, use_hann=True):
    """
    Construit un piano-roll (notes x frames) à partir de la FFT par trames.
    Retourne (mag_notes, times, note_labels) où:
      - mag_notes: (108, n_frames) normalisé [0..1], lignes = C0..B8
      - times: centres des trames (s)
      - note_labels: ['C0','C#0',...,'B8'] (108 entrées)
    """
    # Grille de notes C0..B8
    note_labels = [f"{name}{oct}" for oct in range(0, 9) for name in Tools.NOTE_NAMES]
    n_note_bins = len(note_labels)  # 108

    # Fenêtrage et tramage
    samples_per_window = int(sample_rate * WINDOW_TIME)
    if samples_per_window <= 0:
        raise ValueError("WINDOW_TIME trop petit")
    n_frames = int(len(audio_data) / samples_per_window)
    if n_frames <= 0:
        return np.zeros((n_note_bins, 0)), np.array([]), note_labels

    freqs = np.fft.rfftfreq(samples_per_window, d=1.0 / sample_rate)

    # masque bande utile
    in_band = (freqs >= FREQ_MIN) & (freqs <= FREQ_MAX)

    # tableau de sortie rempli avec -999
    midi_numbers = np.full(freqs.shape, -999, dtype=int)

    # ne convertir qu'aux indices True du masque
    valid_freqs = freqs[in_band]
    midi_vals = np.array([Tools.freq_to_number(float(f)) for f in valid_freqs], dtype=int)
    midi_numbers[in_band] = midi_vals
    note_idx = midi_numbers - 12  # C0 (MIDI 12) -> 0
    valid_bins = (note_idx >= 0) & (note_idx < n_note_bins)

    # Fenêtre (réduit les fuites spectrales)
    window = np.hanning(samples_per_window) if use_hann else None

    # Accumulateur (notes x frames)
    mag_notes = np.zeros((n_note_bins, n_frames), dtype=float)

    # Boucle sur les trames
    for t in range(n_frames):
        start = t * samples_per_window
        end = start + samples_per_window
        frame = audio_data[start:end]
        if len(frame) < samples_per_window:
            # ignore les trames incomplètes en fin d'audio
            break
        if window is not None:
            frame = frame * window

        spec = np.fft.rfft(frame)
        spec_mag = np.abs(spec)

        # Accumulation dans les buckets de notes (vectorisée)
        # On ne prend que les bins valides
        idx_bins = note_idx[valid_bins]
        vals = spec_mag[valid_bins]
        # np.add.at gère les collisions (plusieurs bins sur la même note)
        np.add.at(mag_notes[:, t], idx_bins, vals)

    # Normalisation globale
    mx = mag_notes.max()
    if mx > 0:
        mag_notes /= mx

    # Temps (centre de trame)
    times = (np.arange(n_frames) + 0.5) * (samples_per_window / sample_rate)

    return mag_notes, times, note_labels

def plot_pianoroll_fft(audio_data, sample_rate, audio_bpm, show_beats=False, save_path=None):
    """
    Affiche le piano-roll basé sur la FFT (notes x temps).
    - show_beats=True : axe X en beats (utilise audio_bpm)
    - save_path : si renseigné, sauvegarde le PNG
    """
    mag_notes, times, note_labels = fft_note_buckets_mag(audio_data, sample_rate)
    if mag_notes.size == 0:
        print("Aucune donnée FFT pour le piano-roll.")
        return

    # Axe X: secondes ou beats
    if show_beats:
        x = np.array([Tools.seconds_to_beat(t, audio_bpm) for t in times])
        x_label = f"Temps (beats) — BPM ≈ {int(audio_bpm)}"
    else:
        x = times
        x_label = "Temps (s)"

    fig, ax = plt.subplots(figsize=(14, 6))

    # imshow avec extent pour positionner correctement les axes
    n_bins, n_frames = mag_notes.shape
    x_end = x[-1] if len(x) > 1 else (x[0] + WINDOW_TIME)
    extent = [x[0], x_end, 0, n_bins]
    im = ax.imshow(mag_notes, aspect='auto', origin='lower', extent=extent)  # (pas de couleurs spécifiées)

    # Ticks Y: un label par octave (sur les C)
    yticks_idx = list(range(0, n_bins, 12))
    yticks_lbl = [note_labels[i] for i in yticks_idx]
    ax.set_yticks(yticks_idx)
    ax.set_yticklabels(yticks_lbl)

    # Grilles légères
    for y in yticks_idx:
        ax.axhline(y, linewidth=0.5)
    step = 1.0
    x_max = x[-1] if len(x) else 0
    for gx in np.arange(0, np.ceil(x_max) + 1e-6, step):
        ax.axvline(gx, linewidth=0.5)

    ax.set_xlabel(x_label)
    ax.set_ylabel("Notes (octaves)")
    ax.set_title("Piano-roll (FFT → buckets de notes)")

    plt.tight_layout()
    if save_path:
        plt.savefig(save_path, dpi=150)
        plt.close(fig)
        print(f"Piano-roll FFT sauvegardé → {save_path}")
    else:
        plt.show() 

def plot_pianoroll(audio_data, sample_rate, audio_bpm, use_vqt=False, show_beats=False, save_path=None,
                   threshold=None,           # ex: 0.02  -> masque mag < 2% du max
                   percentile=None):          # ex: 75    -> masque mag < 75e percentile
    """
    Affiche un piano-roll CQT/VQT avec masque sur faibles magnitudes.
      - threshold : seuil absolu (dans [0,1]) appliqué sur la magnitude normalisée
      - percentile : si fourni, on calcule le seuil comme np.percentile(mag, percentile)
      - show_beats : axe X en beats
      - save_path  : si fourni, sauvegarde le PNG
    """
    mag, times, note_labels = wavelet_mag(audio_data, sample_rate, use_vqt=use_vqt)
    if mag.size == 0:
        print("Aucune donnée pour le piano-roll.")
        return

    # Axe X: secondes ou beats
    if show_beats:
        x = np.array([Tools.seconds_to_beat(t, audio_bpm) for t in times])
        x_label = f"Temps (beats) — BPM ≈ {int(audio_bpm)}"
    else:
        x = times
        x_label = "Temps (s)"

    # Détermination du seuil
    if percentile is not None:
        thr = float(np.percentile(mag, percentile))
    elif threshold is not None:
        thr = float(threshold)
    else:
        thr = None

    # Masquage des faibles magnitudes
    data = mag
    cmap = None
    if thr is not None:
        data = np.ma.masked_less(mag, thr)
        import matplotlib.pyplot as plt
        cmap = plt.cm.viridis.copy()   # n'impose pas de couleurs spécifiques
        cmap.set_bad(alpha=0.0)        # valeurs masquées -> transparentes

    # Tracé
    import matplotlib.pyplot as plt
    fig, ax = plt.subplots(figsize=(14, 6))

    n_bins = data.shape[0]
    x_end = x[-1] if len(x) > 1 else (x[0] + WINDOW_TIME)
    extent = [x[0], x_end, 0, n_bins]
    im = ax.imshow(data, aspect='auto', origin='lower', extent=extent, cmap=cmap)

    # Ticks Y: un label par octave (sur chaque C)
    yticks_idx = list(range(0, n_bins, 1))
    yticks_lbl = [note_labels[i] for i in yticks_idx]
    ax.set_yticks(yticks_idx)
    ax.set_yticklabels(yticks_lbl)

    # Grilles légères
    for y in yticks_idx:
        ax.axhline(y, linewidth=0.5)
    step = 1.0
    x_max = x[-1] if len(x) else 0
    for gx in np.arange(0, np.ceil(x_max) + 1e-6, step):
        ax.axvline(gx, linewidth=0.5)

    ax.set_xlabel(x_label)
    ax.set_ylabel("Notes (octaves)")
    title = "Piano-roll Wavelet"
    if thr is not None:
        title += f" (masque < {thr:.3f}{' (perc.)' if percentile is not None else ''})"
    ax.set_title(title)

    plt.tight_layout()
    if save_path:
        plt.savefig(save_path, dpi=150)
        plt.close(fig)
        print(f"Piano-roll sauvegardé → {save_path}")
    else:
        plt.show()

    

if __name__ == "__main__":
    audio_data, sr = librosa.load("audio_in/PinkPanther_Both.mp3")
    print(audio_data)
    bpm, beat_times = estimate_bpm_from_wavelet(audio_data, sr, use_vqt=True)
    print("BPM estimé:", bpm)
    print("Battements (s):", beat_times[:10])

    #plot_pianoroll_fft(audio_data, sr, bpm)
    plot_pianoroll(audio_data, sr, bpm, threshold=0.05)

